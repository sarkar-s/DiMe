{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a sequence of NN models to predict the increment of the inverse CDF with each percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.notebook import tqdm\n",
    "#from tensorflow.keras.layers.merge import Maximum, Minimum\n",
    "\n",
    "#import tensorflow_docs as tfdocs\n",
    "#import tensorflow_docs.plots\n",
    "#import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neurons(qi):\n",
    "    #return int(2**(4 + 0.01*qi*3))\n",
    "\n",
    "    return int(16 + (128 - 16)*(qi-1)/float(98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_cdf(a):\n",
    "    for i in range(a.shape[1]-1,3,-1):\n",
    "        a[:,i] = a[:,i] - a[:,i-1]\n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = './training_data'\n",
    "\n",
    "os.chdir(data_directory)\n",
    "\n",
    "cdf_training_files = glob('*survey.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_training_set = {}\n",
    "\n",
    "c = np.linspace(1,99,99,dtype=int)\n",
    "column_names = [str(ic) for ic in c]\n",
    "\n",
    "gs = np.linspace(1,11,11,dtype=int)\n",
    "g_names = ['g'+str(i) for i in gs]\n",
    "\n",
    "#column_names = ['c']+g_names+['g']+column_names\n",
    "column_names = ['c']+['g']+column_names\n",
    "#print(column_names)\n",
    "\n",
    "for i in range(0,len(cdf_training_files)):\n",
    "    if i==0:\n",
    "        a = pd.read_csv(cdf_training_files[i],header=None).to_numpy()\n",
    "        \n",
    "        concs = np.zeros(shape=(a.shape[0],1))\n",
    "        concs[:,0] = a[:,0]\n",
    "        \n",
    "        response_labels = np.zeros(shape=(a.shape[0],a.shape[0]))\n",
    "\n",
    "        for j in range(0,a.shape[0]):\n",
    "            response_labels[j,:] = a[:,1]\n",
    "         \n",
    "        for j in range(0,a.shape[0]):\n",
    "            if a[j,2]==0.0:\n",
    "                a[j,2] = 0.5*a[j,3]\n",
    "        #print(concs.shape,response_labels.shape,a[:,1:].shape)\n",
    "            \n",
    "        #data_array = np.zeros(shape=(a.shape[0],(a.shape[1]+response_labels.shape[1])))\n",
    "        \n",
    "        #aa = shift_cdf(a)\n",
    "        \n",
    "        #data_array = np.concatenate((concs,aa[:,1:]),axis=1)\n",
    "        mean_array = np.zeros(shape=(a[:,1].shape[0],1))\n",
    "        mean_array[:,0] = a[:,1]\n",
    "        data_array = np.concatenate((concs,mean_array,a[:,3:-1]),axis=1)\n",
    "        \n",
    "        #print(data_array)\n",
    "    else:\n",
    "        a = pd.read_csv(cdf_training_files[i],header=None).to_numpy()\n",
    "        \n",
    "        concs = np.zeros(shape=(a.shape[0],1))\n",
    "        concs[:,0] = a[:,0]\n",
    "        \n",
    "        response_labels = np.zeros(shape=(a.shape[0],a.shape[0]))\n",
    "\n",
    "        for j in range(0,a.shape[0]):\n",
    "            response_labels[j,:] = a[:,1]\n",
    "        \n",
    "        for j in range(0,a.shape[0]):\n",
    "            if a[j,2]==0.0:\n",
    "                a[j,2] = 0.5*a[j,3]\n",
    "            \n",
    "        #aa = shift_cdf(a)\n",
    "            \n",
    "        #this_array = np.concatenate((concs,aa[:,1:]),axis=1)\n",
    "        mean_array = np.zeros(shape=(a[:,1].shape[0],1))\n",
    "        mean_array[:,0] = a[:,1]\n",
    "        this_array = np.concatenate((concs,mean_array,a[:,3:-1]),axis=1)\n",
    "        \n",
    "        #print(i,cdf_training_files[i],this_array.shape,data_array.shape)\n",
    "        #sys.stdout.flush()\n",
    "            \n",
    "        data_array = np.concatenate((data_array,this_array),axis=0)\n",
    "        \n",
    "total_training_set = pd.DataFrame(data_array,index=[i for i in range(data_array.shape[0])],columns=column_names)\n",
    "#total_training_set[i] = pd.read_csv(cdf_training_files[i],names=column_names)\n",
    "#print(total_training_set['g'])\n",
    "print(data_array.shape)\n",
    "#print(list(total_training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = {}\n",
    "training_sets = {}\n",
    "\n",
    "keyset = list(np.linspace(2,data_array.shape[1]-1,99,dtype=int))\n",
    "keyset.reverse()\n",
    "#print(keyset)\n",
    "\n",
    "for i in keyset:\n",
    "    #print(i,column_names[i])\n",
    "    #training_labels[i-13] = total_training_set.pop(column_names[i])\n",
    "    #training_sets[i-13] = copy.deepcopy(total_training_set)\n",
    "    #print(column_names[i])\n",
    "    training_labels[i-1] = total_training_set.pop(column_names[i])\n",
    "    training_sets[i-1] = copy.deepcopy(total_training_set)\n",
    "    \n",
    "    if i>=3:   \n",
    "        #if i==3:\n",
    "        #    print(training_labels[i-2])\n",
    "            \n",
    "        training_labels[i-1] = training_labels[i-1] - total_training_set[str(i-2)]\n",
    "        \n",
    "        #if i==3:\n",
    "        #    print(training_labels[i-2])\n",
    "        #    print(total_training_set[str(i-3)])\n",
    "keyset.reverse()\n",
    "\n",
    "keyset = np.array(keyset,dtype=int) - 2*np.ones(shape=(len(keyset),),dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and save models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '../models/'\n",
    "os.chdir(data_directory)\n",
    "\n",
    "model_name = 'linearly-growing-layers'\n",
    "\n",
    "try:\n",
    "    os.chdir(model_name)\n",
    "except OSError:\n",
    "    os.mkdir(model_name)\n",
    "    os.chdir(model_name)\n",
    "\n",
    "EPOCHS = 5000\n",
    "\n",
    "training_loss = np.zeros(shape=(99,3))\n",
    "validation_loss = np.zeros(shape=(99,3))\n",
    "\n",
    "for i in tqdm(range(1,100)):\n",
    "    #plt.close()\n",
    "    def build_model(qi):\n",
    "        if qi>=0:\n",
    "            n_neurons = neurons(qi)\n",
    "            \n",
    "            print('Neurons: ',n_neurons)\n",
    "            \n",
    "            model = keras.Sequential([\n",
    "            layers.Dense(n_neurons, activation='relu', input_shape=[len(training_sets[i].keys())]),\n",
    "            layers.Dense(n_neurons, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "            ])\n",
    "        \"\"\"\n",
    "        elif qi>=50:\n",
    "            model = keras.Sequential([\n",
    "            layers.Dense(32, activation='relu', input_shape=[len(training_sets[i].keys())]),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            #layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "            ])\n",
    "        elif qi>=25:\n",
    "            model = keras.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=[len(training_sets[i].keys())]),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "            ])\n",
    "        else:\n",
    "            model = keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', input_shape=[len(training_sets[i].keys())]),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "            ])\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = tf.keras.optimizers.RMSprop(0.001,0.9,centered=False)\n",
    "\n",
    "        #model.compile(loss='mse',optimizer=optimizer,metrics=['mae', 'mse'])\n",
    "        \n",
    "        model.compile(loss='mse',optimizer=optimizer,metrics=['mae', 'mse'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    model = build_model(i)\n",
    "\n",
    "    #model.summary()\n",
    "    \n",
    "    j = i - 1\n",
    "    \n",
    "    #history = model.fit(training_sets[i], training_labels[i],\n",
    "    #  epochs=EPOCHS, validation_split = 0.3, verbose=0, batch_size=12, shuffle=True)#callbacks=[tfdocs.modeling.EpochDots()])\n",
    "    \n",
    "    history = model.fit(tf.expand_dims(training_sets[i], axis=-1), training_labels[i],\n",
    "      epochs=EPOCHS, validation_split = 0.3, verbose=0, batch_size=12, shuffle=True)#callbacks=[tfdocs.modeling.EpochDots()])\n",
    "    \n",
    "    loss_array = np.zeros(shape=(len(history.history['loss']),3))\n",
    "    loss_array[:,0] = np.linspace(1,loss_array.shape[0],loss_array.shape[0])\n",
    "    loss_array[:,1] = np.array(history.history['loss'])\n",
    "    loss_array[:,2] = np.array(history.history['val_loss'])\n",
    "    \n",
    "    np.savetxt('loss_array'+str(i)+'.csv',loss_array,delimiter=',')\n",
    "        \n",
    "    training_loss[j,0] = history.history['loss'][0]\n",
    "    training_loss[j,1] = history.history['loss'][-1]\n",
    "    training_loss[j,2] = history.history['loss'][-1]/history.history['loss'][0]\n",
    "    \n",
    "    validation_loss[j,0] = history.history['val_loss'][0]\n",
    "    validation_loss[j,1] = history.history['val_loss'][-1]\n",
    "    validation_loss[j,2] = history.history['val_loss'][-1]/history.history['val_loss'][0]\n",
    "    \n",
    "    print('Loss(training, validation): ',training_loss[j,2],validation_loss[j,2])\n",
    "     \n",
    "    model_name = 'model_'+str(i)+'.h5'\n",
    "    model.save(model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('validation_loss.csv',validation_loss,delimiter=',')\n",
    "np.savetxt('training_loss.csv',training_loss,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
