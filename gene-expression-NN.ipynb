{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a sequence of NN models to predict the increment of the inverse CDF with each percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.notebook import tqdm\n",
    "#from tensorflow.keras.layers.merge import Maximum, Minimum\n",
    "\n",
    "#import tensorflow_docs as tfdocs\n",
    "#import tensorflow_docs.plots\n",
    "#import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neurons(qi):\n",
    "    #return int(2**(4 + 0.01*qi*3))\n",
    "\n",
    "    return int(16 + (128 - 16)*(qi-1)/float(98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_cdf(a):\n",
    "    for i in range(a.shape[1]-1,3,-1):\n",
    "        a[:,i] = a[:,i] - a[:,i-1]\n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = './training_data'\n",
    "\n",
    "os.chdir(data_directory)\n",
    "\n",
    "cdf_training_files = glob('*survey.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 100)\n",
      "['g', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']\n"
     ]
    }
   ],
   "source": [
    "total_training_set = {}\n",
    "\n",
    "c = np.linspace(1,99,99,dtype=int)\n",
    "column_names = [str(ic) for ic in c]\n",
    "\n",
    "gs = np.linspace(1,11,11,dtype=int)\n",
    "g_names = ['g'+str(i) for i in gs]\n",
    "\n",
    "#column_names = ['c']+g_names+['g']+column_names\n",
    "#column_names = ['c']+['g']+column_names\n",
    "column_names = ['g']+column_names\n",
    "#print(column_names)\n",
    "\n",
    "for i in range(0,len(cdf_training_files)):\n",
    "    if i==0:\n",
    "        a = pd.read_csv(cdf_training_files[i],header=None).to_numpy()\n",
    "        \n",
    "        concs = np.zeros(shape=(a.shape[0],1))\n",
    "        concs[:,0] = a[:,0]\n",
    "        \n",
    "        response_labels = np.zeros(shape=(a.shape[0],a.shape[0]))\n",
    "\n",
    "        for j in range(0,a.shape[0]):\n",
    "            response_labels[j,:] = a[:,1]\n",
    "         \n",
    "        for j in range(0,a.shape[0]):\n",
    "            if a[j,2]==0.0:\n",
    "                a[j,2] = 0.5*a[j,3]\n",
    "        #print(concs.shape,response_labels.shape,a[:,1:].shape)\n",
    "            \n",
    "        #data_array = np.zeros(shape=(a.shape[0],(a.shape[1]+response_labels.shape[1])))\n",
    "        \n",
    "        #aa = shift_cdf(a)\n",
    "        \n",
    "        #data_array = np.concatenate((concs,aa[:,1:]),axis=1)\n",
    "        mean_array = np.zeros(shape=(a[:,1].shape[0],1))\n",
    "        mean_array[:,0] = a[:,1]\n",
    "        #data_array = np.concatenate((concs,mean_array,a[:,3:-1]),axis=1)\n",
    "        \n",
    "        data_array = np.concatenate((mean_array,a[:,3:-1]),axis=1)\n",
    "        \n",
    "        #print(data_array)\n",
    "    else:\n",
    "        a = pd.read_csv(cdf_training_files[i],header=None).to_numpy()\n",
    "        \n",
    "        concs = np.zeros(shape=(a.shape[0],1))\n",
    "        concs[:,0] = a[:,0]\n",
    "        \n",
    "        response_labels = np.zeros(shape=(a.shape[0],a.shape[0]))\n",
    "\n",
    "        for j in range(0,a.shape[0]):\n",
    "            response_labels[j,:] = a[:,1]\n",
    "        \n",
    "        for j in range(0,a.shape[0]):\n",
    "            if a[j,2]==0.0:\n",
    "                a[j,2] = 0.5*a[j,3]\n",
    "            \n",
    "        #aa = shift_cdf(a)\n",
    "            \n",
    "        #this_array = np.concatenate((concs,aa[:,1:]),axis=1)\n",
    "        mean_array = np.zeros(shape=(a[:,1].shape[0],1))\n",
    "        mean_array[:,0] = a[:,1]\n",
    "        #this_array = np.concatenate((concs,mean_array,a[:,3:-1]),axis=1)\n",
    "        \n",
    "        this_array = np.concatenate((mean_array,a[:,3:-1]),axis=1)\n",
    "        \n",
    "        #print(i,cdf_training_files[i],this_array.shape,data_array.shape)\n",
    "        #sys.stdout.flush()\n",
    "            \n",
    "        data_array = np.concatenate((data_array,this_array),axis=0)\n",
    "        \n",
    "total_training_set = pd.DataFrame(data_array,index=[i for i in range(data_array.shape[0])],columns=column_names)\n",
    "#total_training_set[i] = pd.read_csv(cdf_training_files[i],names=column_names)\n",
    "#print(total_training_set['g'])\n",
    "print(data_array.shape)\n",
    "print(list(total_training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "training_labels = {}\n",
    "training_sets = {}\n",
    "\n",
    "keyset = list(np.linspace(1,data_array.shape[1]-1,99,dtype=int))\n",
    "keyset.reverse()\n",
    "print(keyset)\n",
    "\n",
    "for i in keyset:\n",
    "    #print(i,column_names[i])\n",
    "    #training_labels[i-13] = total_training_set.pop(column_names[i])\n",
    "    #training_sets[i-13] = copy.deepcopy(total_training_set)\n",
    "    #print(column_names[i])\n",
    "    training_labels[i-1] = total_training_set.pop(column_names[i])\n",
    "    training_sets[i-1] = copy.deepcopy(total_training_set)\n",
    "    \n",
    "    if i>=3:   \n",
    "        #if i==3:\n",
    "        #    print(training_labels[i-2])\n",
    "            \n",
    "        training_labels[i-1] = training_labels[i-1] - total_training_set[str(i-2)]\n",
    "        \n",
    "        #if i==3:\n",
    "        #    print(training_labels[i-2])\n",
    "        #    print(total_training_set[str(i-3)])\n",
    "keyset.reverse()\n",
    "\n",
    "keyset = np.array(keyset,dtype=int) - 2*np.ones(shape=(len(keyset),),dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and save models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9083a320da3402e86d798b7aff907e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons:  14\n",
      "(1200, 1, 1) (1200,)\n",
      "Loss(training, validation):  0.08491944047300827 0.23968204037701404\n",
      "Neurons:  16\n",
      "(1200, 2, 1) (1200,)\n",
      "Loss(training, validation):  0.2264726618138984 2.196529066500324\n",
      "Neurons:  17\n",
      "(1200, 3, 1) (1200,)\n",
      "Loss(training, validation):  0.11757672790763551 0.06792831583477614\n",
      "Neurons:  18\n",
      "(1200, 4, 1) (1200,)\n",
      "Loss(training, validation):  0.03002082957154281 0.047760847522767866\n",
      "Neurons:  19\n",
      "(1200, 5, 1) (1200,)\n",
      "Loss(training, validation):  0.02060230810888833 1.4640174495354736\n",
      "Neurons:  20\n",
      "(1200, 6, 1) (1200,)\n",
      "Loss(training, validation):  0.018289077452782205 0.13449782234366867\n",
      "Neurons:  21\n",
      "(1200, 7, 1) (1200,)\n",
      "Loss(training, validation):  0.0017875199230545766 0.5768888416278062\n",
      "Neurons:  22\n",
      "(1200, 8, 1) (1200,)\n",
      "Loss(training, validation):  0.008102650508279751 0.44576865705317487\n",
      "Neurons:  24\n",
      "(1200, 9, 1) (1200,)\n",
      "Loss(training, validation):  0.0011094274770933983 0.07406849969034784\n",
      "Neurons:  25\n",
      "(1200, 10, 1) (1200,)\n",
      "Loss(training, validation):  6.676889878372637e-05 0.05374936619943743\n",
      "Neurons:  26\n",
      "(1200, 11, 1) (1200,)\n",
      "Loss(training, validation):  0.00045172380352388085 0.0016127693891684908\n",
      "Neurons:  27\n",
      "(1200, 12, 1) (1200,)\n",
      "Loss(training, validation):  0.0005813498496630987 0.041901899168052585\n",
      "Neurons:  28\n",
      "(1200, 13, 1) (1200,)\n",
      "Loss(training, validation):  0.004083241865236215 0.0062977436623744935\n",
      "Neurons:  29\n",
      "(1200, 14, 1) (1200,)\n",
      "Loss(training, validation):  0.0010346966718749126 0.11994344202264949\n",
      "Neurons:  30\n",
      "(1200, 15, 1) (1200,)\n",
      "Loss(training, validation):  4.5624167277529585e-05 0.00039120444051852186\n",
      "Neurons:  32\n",
      "(1200, 16, 1) (1200,)\n",
      "Loss(training, validation):  0.0035813885123705497 0.0327846801989204\n",
      "Neurons:  33\n",
      "(1200, 17, 1) (1200,)\n",
      "Loss(training, validation):  0.000872845375454589 0.0192898250473264\n",
      "Neurons:  34\n",
      "(1200, 18, 1) (1200,)\n",
      "Loss(training, validation):  0.0009479655851642091 0.0009013315832637893\n",
      "Neurons:  35\n",
      "(1200, 19, 1) (1200,)\n",
      "Loss(training, validation):  0.0006164555655431783 0.011996402916455132\n",
      "Neurons:  36\n",
      "(1200, 20, 1) (1200,)\n",
      "Loss(training, validation):  0.000484284813212542 0.01787946869276827\n",
      "Neurons:  37\n",
      "(1200, 21, 1) (1200,)\n",
      "Loss(training, validation):  0.000305634137362721 0.209143392002642\n",
      "Neurons:  38\n",
      "(1200, 22, 1) (1200,)\n",
      "Loss(training, validation):  0.0013331717307441184 0.0007917482058159311\n",
      "Neurons:  40\n",
      "(1200, 23, 1) (1200,)\n",
      "Loss(training, validation):  0.0002423144691360324 0.21926043157988112\n",
      "Neurons:  41\n",
      "(1200, 24, 1) (1200,)\n",
      "Loss(training, validation):  0.0001312245373807584 0.0448684325118157\n",
      "Neurons:  42\n",
      "(1200, 25, 1) (1200,)\n",
      "Loss(training, validation):  0.0008138261462830217 0.009754221300228646\n",
      "Neurons:  43\n",
      "(1200, 26, 1) (1200,)\n",
      "Loss(training, validation):  0.00027090681343988496 0.03366170884525487\n",
      "Neurons:  44\n",
      "(1200, 27, 1) (1200,)\n",
      "Loss(training, validation):  0.002588932703098511 0.004661391895826855\n",
      "Neurons:  45\n",
      "(1200, 28, 1) (1200,)\n",
      "Loss(training, validation):  0.00639130516221349 0.003247518353328401\n",
      "Neurons:  46\n",
      "(1200, 29, 1) (1200,)\n",
      "Loss(training, validation):  0.0003004542842834275 0.0003586084122403909\n",
      "Neurons:  48\n",
      "(1200, 30, 1) (1200,)\n",
      "Loss(training, validation):  0.003775261899360494 0.0005937520950607057\n",
      "Neurons:  49\n",
      "(1200, 31, 1) (1200,)\n",
      "Loss(training, validation):  0.0009434083119535134 9.463801349248297e-05\n",
      "Neurons:  50\n",
      "(1200, 32, 1) (1200,)\n",
      "Loss(training, validation):  0.0007347274734361419 0.19832914665354695\n",
      "Neurons:  51\n",
      "(1200, 33, 1) (1200,)\n",
      "Loss(training, validation):  0.0005563922201695938 0.012279130718795252\n",
      "Neurons:  52\n",
      "(1200, 34, 1) (1200,)\n",
      "Loss(training, validation):  0.0001788093697640041 0.0010981824195919025\n",
      "Neurons:  53\n",
      "(1200, 35, 1) (1200,)\n",
      "Loss(training, validation):  0.0003578833819607153 0.0006912431965875978\n",
      "Neurons:  54\n",
      "(1200, 36, 1) (1200,)\n",
      "Loss(training, validation):  0.0011151301693913568 0.02544669591355951\n",
      "Neurons:  56\n",
      "(1200, 37, 1) (1200,)\n",
      "Loss(training, validation):  0.00029568921906831906 0.010710395101939833\n",
      "Neurons:  57\n",
      "(1200, 38, 1) (1200,)\n",
      "Loss(training, validation):  0.0009046786978036376 0.00682508593235539\n",
      "Neurons:  58\n",
      "(1200, 39, 1) (1200,)\n",
      "Loss(training, validation):  9.936323025883048e-05 0.004726298857571915\n",
      "Neurons:  59\n",
      "(1200, 40, 1) (1200,)\n",
      "Loss(training, validation):  0.0005353155727179392 0.043725344443521434\n",
      "Neurons:  60\n",
      "(1200, 41, 1) (1200,)\n",
      "Loss(training, validation):  0.0004539495735681606 0.000695825876675268\n",
      "Neurons:  61\n",
      "(1200, 42, 1) (1200,)\n",
      "Loss(training, validation):  0.0005234248294671799 0.00014990814745500624\n",
      "Neurons:  62\n",
      "(1200, 43, 1) (1200,)\n",
      "Loss(training, validation):  9.889394506780523e-05 0.03687280679538383\n",
      "Neurons:  64\n",
      "(1200, 44, 1) (1200,)\n",
      "Loss(training, validation):  0.00015161950266418737 0.0030473708734428742\n",
      "Neurons:  65\n",
      "(1200, 45, 1) (1200,)\n",
      "Loss(training, validation):  0.0004175786755950184 0.04579651890855322\n",
      "Neurons:  66\n",
      "(1200, 46, 1) (1200,)\n",
      "Loss(training, validation):  0.00015375764449460327 9.55490696851209e-05\n",
      "Neurons:  67\n",
      "(1200, 47, 1) (1200,)\n",
      "Loss(training, validation):  0.0001599806431337881 8.12430824707437e-05\n",
      "Neurons:  68\n",
      "(1200, 48, 1) (1200,)\n",
      "Loss(training, validation):  0.00030331194192711986 0.00048265194919757727\n",
      "Neurons:  69\n",
      "(1200, 49, 1) (1200,)\n",
      "Loss(training, validation):  0.0002037632148526271 0.0045872572545097145\n",
      "Neurons:  70\n",
      "(1200, 50, 1) (1200,)\n",
      "Loss(training, validation):  7.726318071188802e-05 0.015033077352911019\n",
      "Neurons:  72\n",
      "(1200, 51, 1) (1200,)\n",
      "Loss(training, validation):  9.747253689477649e-05 0.00040607596240415777\n",
      "Neurons:  73\n",
      "(1200, 52, 1) (1200,)\n",
      "Loss(training, validation):  8.494057581328538e-05 0.0007209664162254424\n",
      "Neurons:  74\n",
      "(1200, 53, 1) (1200,)\n",
      "Loss(training, validation):  0.0001256284649934718 0.6537404696379906\n",
      "Neurons:  75\n",
      "(1200, 54, 1) (1200,)\n",
      "Loss(training, validation):  0.0001276487844324256 0.0001270188842986571\n",
      "Neurons:  76\n",
      "(1200, 55, 1) (1200,)\n",
      "Loss(training, validation):  0.00013954198705747937 0.00031707791544660586\n",
      "Neurons:  77\n",
      "(1200, 56, 1) (1200,)\n",
      "Loss(training, validation):  0.0001221373639996541 0.002471104782522956\n",
      "Neurons:  78\n",
      "(1200, 57, 1) (1200,)\n",
      "Loss(training, validation):  0.00032596327765032566 0.00040211082599773734\n",
      "Neurons:  80\n",
      "(1200, 58, 1) (1200,)\n",
      "Loss(training, validation):  9.993951349895268e-05 0.004065822790700595\n",
      "Neurons:  81\n",
      "(1200, 59, 1) (1200,)\n",
      "Loss(training, validation):  9.512619422844498e-05 0.008676024403915593\n",
      "Neurons:  82\n",
      "(1200, 60, 1) (1200,)\n",
      "Loss(training, validation):  0.00022054138690112587 0.19731022364670409\n",
      "Neurons:  83\n",
      "(1200, 61, 1) (1200,)\n",
      "Loss(training, validation):  0.0001358737211389552 0.045668926067815786\n",
      "Neurons:  84\n",
      "(1200, 62, 1) (1200,)\n",
      "Loss(training, validation):  8.916149369664277e-05 0.10456856316843657\n",
      "Neurons:  85\n",
      "(1200, 63, 1) (1200,)\n",
      "Loss(training, validation):  7.461230110038597e-05 0.0028806546449640087\n",
      "Neurons:  86\n",
      "(1200, 64, 1) (1200,)\n",
      "Loss(training, validation):  7.251069572512414e-05 0.022721995094615095\n",
      "Neurons:  88\n",
      "(1200, 65, 1) (1200,)\n",
      "Loss(training, validation):  0.0001627703294037309 9.414867567181694e-05\n",
      "Neurons:  89\n",
      "(1200, 66, 1) (1200,)\n",
      "Loss(training, validation):  0.00011038627090097659 0.04050426875044843\n",
      "Neurons:  90\n",
      "(1200, 67, 1) (1200,)\n",
      "Loss(training, validation):  0.00010163405156079552 0.006186752958030838\n",
      "Neurons:  91\n",
      "(1200, 68, 1) (1200,)\n",
      "Loss(training, validation):  0.00022461975098727618 0.00025236687928617253\n",
      "Neurons:  92\n",
      "(1200, 69, 1) (1200,)\n",
      "Loss(training, validation):  0.00012090511886022413 0.11962822487225447\n",
      "Neurons:  93\n",
      "(1200, 70, 1) (1200,)\n",
      "Loss(training, validation):  0.0001472926306462236 0.0007831006906097223\n",
      "Neurons:  94\n",
      "(1200, 71, 1) (1200,)\n",
      "Loss(training, validation):  8.850720641955307e-05 0.04319699477063442\n",
      "Neurons:  96\n",
      "(1200, 72, 1) (1200,)\n",
      "Loss(training, validation):  0.00017041298564698183 4.393536593494368e-05\n",
      "Neurons:  97\n",
      "(1200, 73, 1) (1200,)\n",
      "Loss(training, validation):  0.00013608926666679836 8.091954142651074e-05\n",
      "Neurons:  98\n",
      "(1200, 74, 1) (1200,)\n",
      "Loss(training, validation):  0.00011366641466777765 0.005436866137510416\n",
      "Neurons:  99\n",
      "(1200, 75, 1) (1200,)\n"
     ]
    }
   ],
   "source": [
    "data_directory = '../models/'\n",
    "os.chdir(data_directory)\n",
    "\n",
    "model_name = 'linearly-growing-layers-2'\n",
    "\n",
    "try:\n",
    "    os.chdir(model_name)\n",
    "except OSError:\n",
    "    os.mkdir(model_name)\n",
    "    os.chdir(model_name)\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "training_loss = np.zeros(shape=(99,3))\n",
    "validation_loss = np.zeros(shape=(99,3))\n",
    "\n",
    "for i in tqdm(range(0,99)):\n",
    "    #plt.close()\n",
    "    def build_model(qi):\n",
    "        if qi>=0:\n",
    "            n_neurons = neurons(qi)\n",
    "            \n",
    "            print('Neurons: ',n_neurons)\n",
    "            \n",
    "            model = keras.Sequential([\n",
    "            layers.Dense(n_neurons, activation='relu', input_shape=[len(training_sets[i].keys())]),\n",
    "            layers.Dense(n_neurons, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "            ])\n",
    "        \"\"\"\n",
    "        elif qi>=50:\n",
    "            model = keras.Sequential([\n",
    "            layers.Dense(32, activation='relu', input_shape=[len(training_sets[i].keys())]),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            #layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "            ])\n",
    "        elif qi>=25:\n",
    "            model = keras.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=[len(training_sets[i].keys())]),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "            ])\n",
    "        else:\n",
    "            model = keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', input_shape=[len(training_sets[i].keys())]),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "            ])\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = tf.keras.optimizers.RMSprop(0.001,0.9,centered=False)\n",
    "\n",
    "        #model.compile(loss='mse',optimizer=optimizer,metrics=['mae', 'mse'])\n",
    "        \n",
    "        model.compile(loss='mse',optimizer=optimizer,metrics=['mae', 'mse'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    model = build_model(i)\n",
    "\n",
    "    #model.summary()\n",
    "    \n",
    "    j = i - 1\n",
    "    \n",
    "    #history = model.fit(training_sets[i], training_labels[i],\n",
    "    #  epochs=EPOCHS, validation_split = 0.3, verbose=0, batch_size=12, shuffle=True)#callbacks=[tfdocs.modeling.EpochDots()])\n",
    "    \n",
    "    recasted_input = tf.expand_dims(training_sets[i], axis=-1)\n",
    "    \n",
    "    print(recasted_input.shape,training_labels[i].shape)\n",
    "    \n",
    "    history = model.fit(recasted_input, training_labels[i], epochs=EPOCHS, validation_split = 0.3, verbose=0, batch_size=12, shuffle=True)#callbacks=[tfdocs.modeling.EpochDots()])\n",
    "    \n",
    "    loss_array = np.zeros(shape=(len(history.history['loss']),3))\n",
    "    loss_array[:,0] = np.linspace(1,loss_array.shape[0],loss_array.shape[0])\n",
    "    loss_array[:,1] = np.array(history.history['loss'])\n",
    "    loss_array[:,2] = np.array(history.history['val_loss'])\n",
    "    \n",
    "    np.savetxt('loss_array'+str(i)+'.csv',loss_array,delimiter=',')\n",
    "        \n",
    "    training_loss[j,0] = history.history['loss'][0]\n",
    "    training_loss[j,1] = history.history['loss'][-1]\n",
    "    training_loss[j,2] = history.history['loss'][-1]/history.history['loss'][0]\n",
    "    \n",
    "    validation_loss[j,0] = history.history['val_loss'][0]\n",
    "    validation_loss[j,1] = history.history['val_loss'][-1]\n",
    "    validation_loss[j,2] = history.history['val_loss'][-1]/history.history['val_loss'][0]\n",
    "    \n",
    "    print('Loss(training, validation): ',training_loss[j,2],validation_loss[j,2])\n",
    "     \n",
    "    model_name = 'model_'+str(i)+'.h5'\n",
    "    model.save(model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('validation_loss.csv',validation_loss,delimiter=',')\n",
    "np.savetxt('training_loss.csv',training_loss,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
