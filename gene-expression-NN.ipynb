{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.notebook import tqdm\n",
    "#from tensorflow.keras.layers.merge import Maximum, Minimum\n",
    "\n",
    "#import tensorflow_docs as tfdocs\n",
    "#import tensorflow_docs.plots\n",
    "#import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_cdf(a):\n",
    "    for i in range(a.shape[1]-1,3,-1):\n",
    "        a[:,i] = a[:,i] - a[:,i-1]\n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = './training_data'\n",
    "\n",
    "os.chdir(data_directory)\n",
    "\n",
    "cdf_training_files = glob('*survey.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 101)\n"
     ]
    }
   ],
   "source": [
    "total_training_set = {}\n",
    "\n",
    "c = np.linspace(1,99,99,dtype=int)\n",
    "column_names = [str(ic) for ic in c]\n",
    "\n",
    "gs = np.linspace(1,11,11,dtype=int)\n",
    "g_names = ['g'+str(i) for i in gs]\n",
    "\n",
    "#column_names = ['c']+g_names+['g']+column_names\n",
    "column_names = ['c']+['g']+column_names\n",
    "#print(column_names)\n",
    "\n",
    "for i in range(0,len(cdf_training_files)):\n",
    "    if i==0:\n",
    "        a = pd.read_csv(cdf_training_files[i],header=None).to_numpy()\n",
    "        \n",
    "        concs = np.zeros(shape=(a.shape[0],1))\n",
    "        concs[:,0] = a[:,0]\n",
    "        \n",
    "        response_labels = np.zeros(shape=(a.shape[0],a.shape[0]))\n",
    "\n",
    "        for j in range(0,a.shape[0]):\n",
    "            response_labels[j,:] = a[:,1]\n",
    "         \n",
    "        for j in range(0,a.shape[0]):\n",
    "            if a[j,2]==0.0:\n",
    "                a[j,2] = 0.5*a[j,3]\n",
    "        #print(concs.shape,response_labels.shape,a[:,1:].shape)\n",
    "            \n",
    "        #data_array = np.zeros(shape=(a.shape[0],(a.shape[1]+response_labels.shape[1])))\n",
    "        \n",
    "        #aa = shift_cdf(a)\n",
    "        \n",
    "        #data_array = np.concatenate((concs,aa[:,1:]),axis=1)\n",
    "        mean_array = np.zeros(shape=(a[:,1].shape[0],1))\n",
    "        mean_array[:,0] = a[:,1]\n",
    "        data_array = np.concatenate((concs,mean_array,a[:,3:-1]),axis=1)\n",
    "        \n",
    "        #print(data_array)\n",
    "    else:\n",
    "        a = pd.read_csv(cdf_training_files[i],header=None).to_numpy()\n",
    "        \n",
    "        concs = np.zeros(shape=(a.shape[0],1))\n",
    "        concs[:,0] = a[:,0]\n",
    "        \n",
    "        response_labels = np.zeros(shape=(a.shape[0],a.shape[0]))\n",
    "\n",
    "        for j in range(0,a.shape[0]):\n",
    "            response_labels[j,:] = a[:,1]\n",
    "        \n",
    "        for j in range(0,a.shape[0]):\n",
    "            if a[j,2]==0.0:\n",
    "                a[j,2] = 0.5*a[j,3]\n",
    "            \n",
    "        #aa = shift_cdf(a)\n",
    "            \n",
    "        #this_array = np.concatenate((concs,aa[:,1:]),axis=1)\n",
    "        mean_array = np.zeros(shape=(a[:,1].shape[0],1))\n",
    "        mean_array[:,0] = a[:,1]\n",
    "        this_array = np.concatenate((concs,mean_array,a[:,3:-1]),axis=1)\n",
    "        \n",
    "        #print(i,cdf_training_files[i],this_array.shape,data_array.shape)\n",
    "        #sys.stdout.flush()\n",
    "            \n",
    "        data_array = np.concatenate((data_array,this_array),axis=0)\n",
    "        \n",
    "total_training_set = pd.DataFrame(data_array,index=[i for i in range(data_array.shape[0])],columns=column_names)\n",
    "#total_training_set[i] = pd.read_csv(cdf_training_files[i],names=column_names)\n",
    "#print(total_training_set['g'])\n",
    "print(data_array.shape)\n",
    "#print(list(total_training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = {}\n",
    "training_sets = {}\n",
    "\n",
    "keyset = list(np.linspace(2,data_array.shape[1]-1,99,dtype=int))\n",
    "keyset.reverse()\n",
    "#print(keyset)\n",
    "\n",
    "for i in keyset:\n",
    "    #print(i,column_names[i])\n",
    "    #training_labels[i-13] = total_training_set.pop(column_names[i])\n",
    "    #training_sets[i-13] = copy.deepcopy(total_training_set)\n",
    "    #print(column_names[i])\n",
    "    training_labels[i-1] = total_training_set.pop(column_names[i])\n",
    "    training_sets[i-1] = copy.deepcopy(total_training_set)\n",
    "    \n",
    "    if i>=3:   \n",
    "        #if i==3:\n",
    "        #    print(training_labels[i-2])\n",
    "            \n",
    "        training_labels[i-1] = training_labels[i-1] - total_training_set[str(i-2)]\n",
    "        \n",
    "        #if i==3:\n",
    "        #    print(training_labels[i-2])\n",
    "        #    print(total_training_set[str(i-3)])\n",
    "keyset.reverse()\n",
    "\n",
    "keyset = np.array(keyset,dtype=int) - 2*np.ones(shape=(len(keyset),),dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and save models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss(training, validation):  0.6121096228382459 2.335917698230069\n",
      "1  completed.\n",
      "Loss(training, validation):  0.7180448476133896 0.6026719647391681\n",
      "2  completed.\n",
      "Loss(training, validation):  0.23154507984075584 0.02291020480047681\n",
      "3  completed.\n",
      "Loss(training, validation):  0.20446610929887024 0.027736565018240924\n",
      "4  completed.\n",
      "Loss(training, validation):  0.09026503043396016 0.006843921662764236\n",
      "5  completed.\n",
      "Loss(training, validation):  0.008207445795734408 0.002736386739885441\n",
      "6  completed.\n",
      "Loss(training, validation):  0.0023944565950288596 0.09929183702662724\n",
      "7  completed.\n",
      "Loss(training, validation):  0.00835044381797208 0.054913659763183006\n",
      "8  completed.\n",
      "Loss(training, validation):  0.0013989165986884472 0.1731679218923829\n",
      "9  completed.\n",
      "Loss(training, validation):  0.00019787539857491325 0.00026188373516625727\n",
      "10  completed.\n",
      "Loss(training, validation):  0.0024761885697409764 0.03619549411250516\n",
      "11  completed.\n",
      "Loss(training, validation):  0.00013296766194312125 0.029302021333767934\n",
      "12  completed.\n",
      "Loss(training, validation):  0.000930056277171313 0.17356656955878516\n",
      "13  completed.\n",
      "Loss(training, validation):  0.00026299777411753256 0.003110362131541391\n",
      "14  completed.\n",
      "Loss(training, validation):  0.0003509812702909796 0.0001513330483185762\n",
      "15  completed.\n",
      "Loss(training, validation):  0.0007457350344904823 0.1450932989535448\n",
      "16  completed.\n",
      "Loss(training, validation):  0.00016578073193607523 0.00017537863747605766\n",
      "17  completed.\n",
      "Loss(training, validation):  0.0010982294549322213 0.07697697556714289\n",
      "18  completed.\n",
      "Loss(training, validation):  0.0015053761380390072 0.04456578637061725\n",
      "19  completed.\n",
      "Loss(training, validation):  0.000463137778685852 0.008325513841557153\n",
      "20  completed.\n",
      "Loss(training, validation):  9.506507103208315e-05 0.0371128455030647\n",
      "21  completed.\n",
      "Loss(training, validation):  0.0006034479687963001 0.008899089384091676\n",
      "22  completed.\n",
      "Loss(training, validation):  0.0003456358827088462 0.004720672843421446\n",
      "23  completed.\n",
      "Loss(training, validation):  0.00012881356482453252 0.012915065365526404\n",
      "24  completed.\n",
      "Loss(training, validation):  4.542306487072583e-05 0.000563402789099193\n",
      "25  completed.\n",
      "Loss(training, validation):  0.0011179323068605813 0.5514664786421588\n",
      "26  completed.\n",
      "Loss(training, validation):  0.00042732974475815673 0.0033287260306159015\n",
      "27  completed.\n",
      "Loss(training, validation):  0.013951220334051854 0.06088542222248538\n",
      "28  completed.\n",
      "Loss(training, validation):  0.0013926148555704668 0.01068375158229217\n",
      "29  completed.\n",
      "Loss(training, validation):  0.00024036130901352036 3.995174044314991e-05\n",
      "30  completed.\n",
      "Loss(training, validation):  0.00047048485006856954 0.09225613782998787\n",
      "31  completed.\n",
      "Loss(training, validation):  7.021305351678234e-05 0.0005907134627968154\n",
      "32  completed.\n",
      "Loss(training, validation):  0.0001504963307584773 0.003261187031820359\n",
      "33  completed.\n",
      "Loss(training, validation):  9.396293375428802e-05 0.003679825836887153\n",
      "34  completed.\n",
      "Loss(training, validation):  6.554525316866715e-05 0.002289433324152274\n",
      "35  completed.\n",
      "Loss(training, validation):  0.004786659577337121 0.31244845420729883\n",
      "36  completed.\n",
      "Loss(training, validation):  1.943546714744055e-05 0.021140333271161736\n",
      "37  completed.\n",
      "Loss(training, validation):  2.467934286464756e-05 0.008737774064809382\n",
      "38  completed.\n",
      "Loss(training, validation):  0.00030617970621738883 0.010632905007852652\n",
      "39  completed.\n",
      "Loss(training, validation):  2.367382248851646e-05 0.0010133706728508742\n",
      "40  completed.\n",
      "Loss(training, validation):  9.544320529975274e-05 0.002455825881274763\n",
      "41  completed.\n",
      "Loss(training, validation):  8.794469868403261e-05 6.862129806542434e-05\n",
      "42  completed.\n",
      "Loss(training, validation):  1.574924222355487e-05 8.361392204330123e-06\n",
      "43  completed.\n",
      "Loss(training, validation):  0.0003479749200123974 0.0009694766175953689\n",
      "44  completed.\n",
      "Loss(training, validation):  8.615097063760593e-05 0.0050706437098926565\n",
      "45  completed.\n",
      "Loss(training, validation):  6.95194555922638e-05 0.02447746475500669\n",
      "46  completed.\n",
      "Loss(training, validation):  0.0001371111819323022 0.0013963612806730938\n",
      "47  completed.\n",
      "Loss(training, validation):  0.00010216800381681348 0.015996318708705392\n",
      "48  completed.\n",
      "Loss(training, validation):  6.521313775308445e-05 0.011261265265280318\n",
      "49  completed.\n",
      "Loss(training, validation):  5.969305096544686e-05 0.0007809622414276862\n",
      "50  completed.\n",
      "Loss(training, validation):  2.7089850210795604e-05 0.0024135021730199596\n",
      "51  completed.\n",
      "Loss(training, validation):  2.7493591659504317e-05 0.01086233953095531\n",
      "52  completed.\n",
      "Loss(training, validation):  0.00011058791866873645 0.0018645269537411722\n",
      "53  completed.\n",
      "Loss(training, validation):  3.6100964840219625e-05 0.0003955428863716421\n",
      "54  completed.\n",
      "Loss(training, validation):  5.732286028461654e-05 0.11499567178609774\n",
      "55  completed.\n",
      "Loss(training, validation):  0.00015162371224549138 0.022858620997177493\n",
      "56  completed.\n",
      "Loss(training, validation):  5.039349401971235e-05 0.0036228691959410333\n",
      "57  completed.\n",
      "Loss(training, validation):  0.0006396967479967102 0.5907386506016353\n",
      "58  completed.\n",
      "Loss(training, validation):  0.00025762976358573305 0.0014398151459837961\n",
      "59  completed.\n",
      "Loss(training, validation):  0.0007310797277221863 0.025804574393006766\n",
      "60  completed.\n",
      "Loss(training, validation):  2.7645773704446848e-05 0.005015790990852944\n",
      "61  completed.\n",
      "Loss(training, validation):  0.0001597038885553631 0.410723103776372\n",
      "62  completed.\n",
      "Loss(training, validation):  0.00038354329362255257 0.01563310148300474\n",
      "63  completed.\n"
     ]
    }
   ],
   "source": [
    "data_directory = '../models/'\n",
    "os.chdir(data_directory)\n",
    "\n",
    "model_name = '3-layers-64-32-16'\n",
    "\n",
    "try:\n",
    "    os.chdir(model_name)\n",
    "except OSError:\n",
    "    os.mkdir(model_name)\n",
    "    os.chdir(model_name)\n",
    "\n",
    "EPOCHS = 5000\n",
    "\n",
    "training_loss = np.zeros(shape=(99,3))\n",
    "validation_loss = np.zeros(shape=(99,3))\n",
    "\n",
    "for i in range(1,100):\n",
    "    def build_model(qi):\n",
    "        if qi>=0:\n",
    "            model = keras.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=[len(training_sets[i].keys())]),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(16, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "            ])\n",
    "        \"\"\"\n",
    "        elif qi>=50:\n",
    "            model = keras.Sequential([\n",
    "            layers.Dense(32, activation='relu', input_shape=[len(training_sets[i].keys())]),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            #layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "            ])\n",
    "        elif qi>=25:\n",
    "            model = keras.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=[len(training_sets[i].keys())]),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "            ])\n",
    "        else:\n",
    "            model = keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', input_shape=[len(training_sets[i].keys())]),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "            ])\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = tf.keras.optimizers.RMSprop(0.002,0.8,centered=True)\n",
    "\n",
    "        model.compile(loss='mse',optimizer=optimizer,metrics=['mae', 'mse'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    model = build_model(i)\n",
    "\n",
    "    #model.summary()\n",
    "    \n",
    "    j = i - 1\n",
    "    \n",
    "    history = model.fit(training_sets[i], training_labels[i],\n",
    "      epochs=EPOCHS, validation_split = 0.2, verbose=0, batch_size=12, shuffle=True)#callbacks=[tfdocs.modeling.EpochDots()])\n",
    "    \n",
    "    loss_array = np.zeros(shape=(len(history.history['loss']),3))\n",
    "    loss_array[:,0] = np.linspace(1,loss_array.shape[0],loss_array.shape[0])\n",
    "    loss_array[:,1] = np.array(history.history['loss'])\n",
    "    loss_array[:,2] = np.array(history.history['val_loss'])\n",
    "\n",
    "    np.savetxt('loss_array'+str(i)+'.csv',loss_array,delimiter=',')\n",
    "        \n",
    "    training_loss[j,0] = history.history['loss'][0]\n",
    "    training_loss[j,1] = history.history['loss'][-1]\n",
    "    training_loss[j,2] = history.history['loss'][-1]/history.history['loss'][0]\n",
    "    \n",
    "    validation_loss[j,0] = history.history['val_loss'][0]\n",
    "    validation_loss[j,1] = history.history['val_loss'][-1]\n",
    "    validation_loss[j,2] = history.history['val_loss'][-1]/history.history['val_loss'][0]\n",
    "    \n",
    "    print('Loss(training, validation): ',training_loss[j,2],validation_loss[j,2])\n",
    "     \n",
    "    model_name = 'model_'+str(i)+'.h5'\n",
    "    model.save(model_name)\n",
    "    \n",
    "    print(i,' completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('validation_loss.csv',validation_loss,delimiter=',')\n",
    "np.savetxt('training_loss.csv',training_loss,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
